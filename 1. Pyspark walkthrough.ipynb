{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "`PySpark` let's you work in a Spark cluster using python. To begin with you need to connect to a Spark cluster. For the purpose of this walkthrough, we won't connect to any real cluster, and all calculation will be done locally.\n",
    "\n",
    "The procedures for working with data, doing calculations and so on are analogous to a real cluster.\n",
    "\n",
    "## Connecting to a cluster\n",
    "\n",
    "In order to create a connection with a cluster, you have to create an instance of `SparkContext()`. You can pass several oprional parameters and configurations that we're not going to discuss in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore your cluster, work with the SparkSession\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. The Spark DataFrame is an obstraction over RDDs that was designed to behave a lot like a SQL table. They easier to understand and also more optimized for complicated operations than RDDs.\n",
    "\n",
    "## The spark session\n",
    "To start working with Spark tables (DataFrames), you first have to create a `SparkSession` object from your `SparkContext`. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List existing tables\n",
    "\n",
    "The `SparkSession` has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n",
    "\n",
    "The `.listTables()` method returns the names of all the tables in your cluster as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='merchants', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='products', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This import and call to create_temp_tables will be explained below\n",
    "import helpers\n",
    "\n",
    "helpers.create_temp_tables(spark)\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying tables\n",
    "\n",
    "One of the advantages of the DataFrame interface is that you can run SQL queries on the tables in your Spark cluster.\n",
    "\n",
    "As you saw in the previous cell, one of the tables in your cluster is the merchants table. This table contains a row for merchant in our system.\n",
    "\n",
    "Running a query on this table is as easy as using the `.sql()` method on your `SparkSession`. This method takes a string containing the query and returns a `DataFrame` with the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of sql() return object is a DataFrame: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----------+---------+--------------------+---------+\n",
      "|merchant_id|     name|               email|month_cpv|\n",
      "+-----------+---------+--------------------+---------+\n",
      "|          1|  Jon Doe|    jon@nonsense.com|    239.0|\n",
      "|          2| Jane Doe|   jane@nonsense.com|    354.6|\n",
      "|          3|No Profit|noprofit@nonsense...|      0.0|\n",
      "|          4|  Mr Rich|   rich@nonsense.com|  12345.0|\n",
      "+-----------+---------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merchants = spark.sql('SELECT * FROM merchants')\n",
    "print('Data type of sql() return object is a DataFrame: {}'.format(type(merchants)))\n",
    "merchants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same with the products table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+-----------+\n",
      "|product_id|product_name|price|merchant_id|\n",
      "+----------+------------+-----+-----------+\n",
      "|         1|       paper|   10|          1|\n",
      "|         2|       glass|  100|          1|\n",
      "|         3|       watch| 1900|          4|\n",
      "|         4|       phone| 7000|          4|\n",
      "|         5|       mouse|  200|          4|\n",
      "|         6|       shirt|  300|          2|\n",
      "|         7|      jacket|  900|          2|\n",
      "|         8|        beer|   39|          3|\n",
      "|         9|      coffee|   32|          3|\n",
      "|        10|   ice cream|   30|          3|\n",
      "+----------+------------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = spark.sql('SELECT * FROM products')\n",
    "products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run any (or most of) SQL queries against spark `DataFrames`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+---------+\n",
      "|merchant_id|     name|               email|month_cpv|\n",
      "+-----------+---------+--------------------+---------+\n",
      "|          1|  Jon Doe|    jon@nonsense.com|    239.0|\n",
      "|          3|No Profit|noprofit@nonsense...|      0.0|\n",
      "+-----------+---------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "low_cpv = spark.sql('SELECT * FROM merchants WHERE month_cpv < 300')\n",
    "low_cpv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note is that you can \"pandify\" your spark DataFrames. This will give you all the power of pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>month_cpv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jon Doe</td>\n",
       "      <td>jon@nonsense.com</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>jane@nonsense.com</td>\n",
       "      <td>354.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No Profit</td>\n",
       "      <td>noprofit@nonsense.com</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mr Rich</td>\n",
       "      <td>rich@nonsense.com</td>\n",
       "      <td>12345.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   merchant_id       name                  email  month_cpv\n",
       "0            1    Jon Doe       jon@nonsense.com      239.0\n",
       "1            2   Jane Doe      jane@nonsense.com      354.6\n",
       "2            3  No Profit  noprofit@nonsense.com        0.0\n",
       "3            4    Mr Rich      rich@nonsense.com    12345.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchants_pd = merchants.toPandas()\n",
    "merchants_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new data\n",
    "\n",
    "The `.createDataFrame()` method takes a pandas DataFrame and returns a Spark DataFrame.\n",
    "\n",
    "The output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
    "\n",
    "To access the data in this way, you have to save it as a temporary table.\n",
    "\n",
    "You can do this using the `.createTempView()` Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\n",
    "\n",
    "There is also the method `.createOrReplaceTempView()`. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined.\n",
    "\n",
    "This is actually how we've generated the merchants and products tables. Only that instead of creating the tables from pandas DataFrames, we've done it from CSV files.\n",
    "\n",
    "Take a look at the code for `create_temp_tables` in `helpers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def create_temp_tables(spark):\n",
      "    \"\"\" Create some temporal tables for data manipulation\n",
      "\n",
      "    Params:\n",
      "        spark - Spark session\n",
      "    \"\"\"\n",
      "    merchants = spark.read.csv('data/merchants.csv', header=True, inferSchema=True)\n",
      "    products = spark.read.csv('data/products.csv', header=True, inferSchema=True)\n",
      "    merchants.createOrReplaceTempView('merchants')\n",
      "    products.createOrReplaceTempView('products')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(helpers.create_temp_tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More data manipulation\n",
    "\n",
    "## Create new columns\n",
    "\n",
    "In Spark you can do this using the `.withColumn()` method, which takes two arguments. First, a string with the name of your new column, and second the new column itself.\n",
    "\n",
    "The new column must be an object of class Column. You can do this by extracting a column from your DataFrame using `df.colName`.\n",
    "\n",
    "Updating a Spark DataFrame is different than working in pandas because the Spark DataFrame is immutable. This means that it can't be changed, and so columns can't be updated in place.\n",
    "\n",
    "Thus, all these methods return a new DataFrame. To overwrite the original DataFrame you must reassign the returned DataFrame using the method like so:\n",
    "\n",
    "    df = df.withColumn(\"newCol\", df.oldCol + 1)\n",
    "\n",
    "The above code creates a DataFrame with the same columns as df plus a new column, newCol, where every entry is equal to the corresponding entry from oldCol, plus one.\n",
    "\n",
    "Let's try and add an extra column to our products table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+-----------+-----------+\n",
      "|product_id|product_name|price|merchant_id|10_discount|\n",
      "+----------+------------+-----+-----------+-----------+\n",
      "|         1|       paper|   10|          1|        9.0|\n",
      "|         2|       glass|  100|          1|       90.0|\n",
      "|         3|       watch| 1900|          4|     1710.0|\n",
      "|         4|       phone| 7000|          4|     6300.0|\n",
      "|         5|       mouse|  200|          4|      180.0|\n",
      "|         6|       shirt|  300|          2|      270.0|\n",
      "|         7|      jacket|  900|          2|      810.0|\n",
      "|         8|        beer|   39|          3|       35.1|\n",
      "|         9|      coffee|   32|          3|       28.8|\n",
      "|        10|   ice cream|   30|          3|       27.0|\n",
      "+----------+------------+-----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_products = products.withColumn('10_discount', products.price*0.9)\n",
    "new_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting, filtering, aggregating, grouping, joining... SQL all the things!\n",
    "\n",
    "Instead of using the spark session to run SQL-like queries, you can also use the built in methods in the spark DataFrames.\n",
    "\n",
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+\n",
      "|     name|               email|month_cpv|\n",
      "+---------+--------------------+---------+\n",
      "|  Jon Doe|    jon@nonsense.com|    239.0|\n",
      "| Jane Doe|   jane@nonsense.com|    354.6|\n",
      "|No Profit|noprofit@nonsense...|      0.0|\n",
      "|  Mr Rich|   rich@nonsense.com|  12345.0|\n",
      "+---------+--------------------+---------+\n",
      "\n",
      "+---------+--------------------+\n",
      "|     name|               email|\n",
      "+---------+--------------------+\n",
      "|  Jon Doe|    jon@nonsense.com|\n",
      "| Jane Doe|   jane@nonsense.com|\n",
      "|No Profit|noprofit@nonsense...|\n",
      "|  Mr Rich|   rich@nonsense.com|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Syntax 1, with strings\n",
    "names = merchants.select('name', 'email', 'month_cpv')\n",
    "names.show()\n",
    "\n",
    "# Syntax 2, with DataFrame column names\n",
    "emails = merchants.select(merchants.name, merchants.email)\n",
    "emails.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use `selectExpr` to select new columns as operations with other columns.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|product_name|price|discounted|\n",
      "+------------+-----+----------+\n",
      "|       paper|   10|       9.0|\n",
      "|       glass|  100|      90.0|\n",
      "|       watch| 1900|    1710.0|\n",
      "|       phone| 7000|    6300.0|\n",
      "|       mouse|  200|     180.0|\n",
      "|       shirt|  300|     270.0|\n",
      "|      jacket|  900|     810.0|\n",
      "|        beer|   39|      35.1|\n",
      "|      coffee|   32|      28.8|\n",
      "|   ice cream|   30|      27.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.selectExpr('product_name', 'price', 'price*0.9 as discounted').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or create an alias column and use it for selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|product_name|price|discounted|\n",
      "+------------+-----+----------+\n",
      "|       paper|   10|       9.0|\n",
      "|       glass|  100|      90.0|\n",
      "|       watch| 1900|    1710.0|\n",
      "|       phone| 7000|    6300.0|\n",
      "|       mouse|  200|     180.0|\n",
      "|       shirt|  300|     270.0|\n",
      "|      jacket|  900|     810.0|\n",
      "|        beer|   39|      35.1|\n",
      "|      coffee|   32|      28.8|\n",
      "|   ice cream|   30|      27.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "discount = (products.price*0.9).alias('discounted')\n",
    "products.select('product_name', 'price', discount).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering \n",
    "You can also apply filters to your data, and chain them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----------------+---------+\n",
      "|merchant_id|    name|            email|month_cpv|\n",
      "+-----------+--------+-----------------+---------+\n",
      "|          1| Jon Doe| jon@nonsense.com|    239.0|\n",
      "|          2|Jane Doe|jane@nonsense.com|    354.6|\n",
      "+-----------+--------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_by_j = merchants.name.contains('J')\n",
    "merchants.filter(filter_by_j).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------------+---------+\n",
      "|merchant_id|   name|           email|month_cpv|\n",
      "+-----------+-------+----------------+---------+\n",
      "|          1|Jon Doe|jon@nonsense.com|    239.0|\n",
      "+-----------+-------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_low_cpv = merchants.month_cpv < 300\n",
    "merchants.filter(filter_by_j).filter(filter_low_cpv).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating\n",
    "\n",
    "All of the common aggregation methods `.min()`, `.max()` and `.count()` are `GroupedData` methods. These are created by calling the `.groupBy()` DataFrame method. For example, to find the product with the lower price, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(price)|\n",
      "+----------+\n",
      "|        10|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.groupBy().min('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(price)|\n",
      "+----------+\n",
      "|      55.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average price products of merchant 1\n",
    "products.filter(products.merchant_id == 1).groupBy().avg('price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "You can also pass arguments to the `groupBy` method to group your data, and run aggregations on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|merchant_id|        avg(price)|\n",
      "+-----------+------------------+\n",
      "|          1|              55.0|\n",
      "|          3|33.666666666666664|\n",
      "|          4|3033.3333333333335|\n",
      "|          2|             600.0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average product price per merchant\n",
    "products.groupBy('merchant_id').avg('price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the GroupedData methods, there is also the `.agg()` method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the `pyspark.sql.functions` submodule.\n",
    "\n",
    "This submodule contains many useful functions for computing things like standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|merchant_id|stddev_samp(price)|\n",
      "+-----------+------------------+\n",
      "|          1| 63.63961030678928|\n",
      "|          3| 4.725815626252609|\n",
      "|          4|3538.8321990924255|\n",
      "|          2|424.26406871192853|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Standard deviation of product prices by merchant\n",
    "products.groupBy('merchant_id').agg(F.stddev('price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining\n",
    "\n",
    "Finally, you can also join tables in spark, and it's easy!\n",
    "\n",
    "Joins are performed using the DataFrame method `.join()`. This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, `on`, is the name of the key column(s) as a string. **The names of the key column(s) must be the same in each table**. The third argument, `how`, specifies the kind of join to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+---------+----------+------------+-----+\n",
      "|merchant_id|     name|               email|month_cpv|product_id|product_name|price|\n",
      "+-----------+---------+--------------------+---------+----------+------------+-----+\n",
      "|          1|  Jon Doe|    jon@nonsense.com|    239.0|         2|       glass|  100|\n",
      "|          1|  Jon Doe|    jon@nonsense.com|    239.0|         1|       paper|   10|\n",
      "|          2| Jane Doe|   jane@nonsense.com|    354.6|         7|      jacket|  900|\n",
      "|          2| Jane Doe|   jane@nonsense.com|    354.6|         6|       shirt|  300|\n",
      "|          3|No Profit|noprofit@nonsense...|      0.0|        10|   ice cream|   30|\n",
      "|          3|No Profit|noprofit@nonsense...|      0.0|         9|      coffee|   32|\n",
      "|          3|No Profit|noprofit@nonsense...|      0.0|         8|        beer|   39|\n",
      "|          4|  Mr Rich|   rich@nonsense.com|  12345.0|         5|       mouse|  200|\n",
      "|          4|  Mr Rich|   rich@nonsense.com|  12345.0|         4|       phone| 7000|\n",
      "|          4|  Mr Rich|   rich@nonsense.com|  12345.0|         3|       watch| 1900|\n",
      "+-----------+---------+--------------------+---------+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merchants.join(products, on='merchant_id', how='leftouter').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
